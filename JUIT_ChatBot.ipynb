{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TusharPaul01/ChatBot-ML-LSTM-/blob/main/JUIT_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "# uplink_key = getpass('Enter your Uplink key: ')\n",
        "\n",
        "uplink_key = 'server_TFKWGAGVLZ3VWY6T5KD6KWBV-7XQ4BDMT2GO2DEOD'"
      ],
      "metadata": {
        "id": "6_6b4TmKKU21"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anvil-uplink"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "tubz8xd0KjY-",
        "outputId": "f4b140de-f01f-4912-cb46-9cefdf704f16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.4.2-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/90.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m81.9/90.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argparse (from anvil-uplink)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.16.0)\n",
            "Collecting ws4py (from anvil-uplink)\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45228 sha256=a5c5c182c09816a92b75d3a072cf3471e924974ca7a8d0f3b54964b71416cd75\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/7c/ad/d9c746276bf024d44296340869fcb169f1e5d80fb147351a57\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.4.2 argparse-1.4.0 ws4py-0.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anvil.server\n",
        "anvil.server.connect('server_TFKWGAGVLZ3VWY6T5KD6KWBV-7XQ4BDMT2GO2DEOD')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQUX5JMDKypt",
        "outputId": "8c5532aa-287e-4525-95d9-fba472f01ad6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"ChatBot\" as SERVER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o2JRRpWA2wHZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from JSON file\n",
        "with open('intents.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# extract text and intent from data\n",
        "texts = []\n",
        "intents = []\n",
        "for intent in data['intents']:\n",
        "    for text in intent['text']:\n",
        "        texts.append(text)\n",
        "        intents.append(intent['intent'])\n",
        "\n",
        "# tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "encoded_texts = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# save tokenizer\n",
        "import pickle\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# pad sequences to have equal length\n",
        "max_len = max([len(x) for x in encoded_texts])\n",
        "padded_texts = pad_sequences(encoded_texts, maxlen=max_len, padding='post')\n",
        "\n",
        "# create label encoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# fit and transform the intents to integer labels\n",
        "encoded_intents = le.fit_transform(intents)\n",
        "\n",
        "# get the number of unique labels\n",
        "num_intents = len(le.classes_)\n",
        "\n",
        "# apply one-hot encoding to the integer labels\n",
        "encoded_intents = tf.one_hot(encoded_intents, depth=num_intents)\n",
        "\n",
        "# define model architecture\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_len)(input_layer)\n",
        "lstm_layer = LSTM(128)(embedding_layer)\n",
        "output_layer = Dense(num_intents, activation='softmax')(lstm_layer)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train model\n",
        "model.fit(padded_texts, encoded_intents, epochs=50, batch_size=16)\n",
        "\n",
        "# save model\n",
        "model.save('chatbot_model34.h5')\n"
      ],
      "metadata": {
        "id": "XQsI0heL8nOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e889bf-dd32-4068-bf45-c05caa289a06"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "26/26 [==============================] - 4s 15ms/step - loss: 3.6190 - accuracy: 0.0519\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 3.5178 - accuracy: 0.0667\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 0s 16ms/step - loss: 3.3207 - accuracy: 0.1210\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 2.8070 - accuracy: 0.1852\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 23ms/step - loss: 2.4014 - accuracy: 0.2741\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 28ms/step - loss: 2.0073 - accuracy: 0.3951\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 32ms/step - loss: 1.5839 - accuracy: 0.5358\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 24ms/step - loss: 1.2500 - accuracy: 0.6074\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 1s 27ms/step - loss: 0.9588 - accuracy: 0.7284\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 26ms/step - loss: 0.7251 - accuracy: 0.8148\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 30ms/step - loss: 0.5637 - accuracy: 0.8790\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 23ms/step - loss: 0.4164 - accuracy: 0.9111\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 26ms/step - loss: 0.3444 - accuracy: 0.9037\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 24ms/step - loss: 0.2552 - accuracy: 0.9531\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 25ms/step - loss: 0.1745 - accuracy: 0.9753\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 1s 44ms/step - loss: 0.1373 - accuracy: 0.9778\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 52ms/step - loss: 0.0911 - accuracy: 0.9901\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.0717 - accuracy: 0.9926\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 41ms/step - loss: 0.0632 - accuracy: 0.9901\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 32ms/step - loss: 0.0570 - accuracy: 0.9901\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 1s 27ms/step - loss: 0.0498 - accuracy: 0.9926\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 26ms/step - loss: 0.0447 - accuracy: 0.9901\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 26ms/step - loss: 0.0412 - accuracy: 0.9926\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 24ms/step - loss: 0.0411 - accuracy: 0.9901\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 0.0372 - accuracy: 0.9901\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0355 - accuracy: 0.9877\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 0.0348 - accuracy: 0.9926\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 0s 16ms/step - loss: 0.0327 - accuracy: 0.9901\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 0s 16ms/step - loss: 0.0429 - accuracy: 0.9852\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0321 - accuracy: 0.9877\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0286 - accuracy: 0.9877\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 0s 14ms/step - loss: 0.0252 - accuracy: 0.9951\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 0.0234 - accuracy: 0.9926\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 0.0244 - accuracy: 0.9877\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0305 - accuracy: 0.9926\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0524 - accuracy: 0.9877\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0902 - accuracy: 0.9753\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0638 - accuracy: 0.9802\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0731 - accuracy: 0.9728\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0794 - accuracy: 0.9802\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0782 - accuracy: 0.9728\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 0s 17ms/step - loss: 0.1645 - accuracy: 0.9457\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 20ms/step - loss: 0.1553 - accuracy: 0.9531\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 20ms/step - loss: 0.1522 - accuracy: 0.9580\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 21ms/step - loss: 0.0646 - accuracy: 0.9877\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 23ms/step - loss: 0.0468 - accuracy: 0.9877\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 24ms/step - loss: 0.0477 - accuracy: 0.9852\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 0s 18ms/step - loss: 0.0466 - accuracy: 0.9877\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0279 - accuracy: 0.9926\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0209 - accuracy: 0.9901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# load data from JSON file\n",
        "with open('intents_final.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# extract text and intent from data\n",
        "texts = []\n",
        "intents = []\n",
        "for intent in data['intents']:\n",
        "    for text in intent['text']:\n",
        "        texts.append(text)\n",
        "        intents.append(intent['intent'])\n",
        "\n",
        "# tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# load saved model\n",
        "model = load_model('chatbot_model34.h5')\n",
        "\n",
        "# define maximum sequence length\n",
        "max_len = model.input_shape[1]\n",
        "\n",
        "# create label encoder object\n",
        "le = LabelEncoder()\n",
        "le.fit(intents)\n",
        "\n",
        "# create inverse mapping of label encoder for intent prediction\n",
        "intent_mapping = {i: label for i, label in enumerate(le.classes_)}\n",
        "\n",
        "# start chatbot interaction\n"
      ],
      "metadata": {
        "id": "7N2ht1VGEKwl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@anvil.server.callable\n",
        "def chatbot(sentence):\n",
        "  while True:\n",
        "    # get user input\n",
        "      user_input = sentence.lower().strip()\n",
        "\n",
        "      # check if user wants to quit\n",
        "      if user_input == 'quit':\n",
        "        break\n",
        "\n",
        "      # encode user input text\n",
        "      encoded_input = tokenizer.texts_to_sequences([user_input])\n",
        "      padded_input = pad_sequences(encoded_input, maxlen=max_len, padding='post')\n",
        "\n",
        "      # predict intent\n",
        "      intent_prob = model.predict(padded_input)[0]\n",
        "      intent_idx = np.argmax(intent_prob)\n",
        "      intent_label = le.inverse_transform([intent_idx])[0]\n",
        "\n",
        "      # retrieve response\n",
        "      for intent in data['intents']:\n",
        "        if intent['intent'] == intent_label:\n",
        "            response = np.random.choice(intent['responses'])\n",
        "            return response\n",
        "            break\n",
        ""
      ],
      "metadata": {
        "id": "vfGAGYgzVtEv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anvil.server.wait_forever()"
      ],
      "metadata": {
        "id": "QcrdyDWbMdWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64f24d8-5397-49b3-8493-37ac35dd4237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 691ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ]
    }
  ]
}